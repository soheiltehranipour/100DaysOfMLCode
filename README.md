# 100DaysOfMLCode
My journey to Applied Machine Learning has been started today (09/30/2018) based on #100daysofmlcode and [sirajraval](https://github.com/llSourcell) . It is all about dedicating at least 1 hour studying or coding Machine Learning daily. Iâ€™m going to post the details in my github and also in our official website (www.iran-machinelearning.ir)
## [Day 000 - Introduction](https://github.com/soheiltehranipour/100DaysOfMLCode/blob/master/Day%20000-%20Intro.ipynb)

Introduction and some How To's for starting Data Science and Machine Learning with a little of Python Programming and its data structures.

## [Day 001 - Numpy](https://github.com/soheiltehranipour/100DaysOfMLCode/blob/master/Day%20001-%20Numpy.ipynb)

Numpy From Scratch. 

## [Day 002 - Pandas](https://github.com/soheiltehranipour/100DaysOfMLCode/blob/master/Day%20002%20-%20Pandas.ipynb)

Now it's time to get our hands dirty and doing some Data manipulation with Pandas. Titanic Dataset is used here.

## Day 003 - Visualization (Matplotlib & Seaborn)

## Day 004 - Linear Regression

## Day 005 - Gradient Descent

This is one the most popular algorithms to perform optimization and by far the most common way to optimize Neural Networks which i will cover in the following days.
Sebastian ruder wrote a very good paper about gradient descent and i loved it. 

## [Day 006 - XGBoost](http://iran-machinelearning.ir/%d8%a7%d9%84%da%af%d9%88%d8%b1%db%8c%d8%aa%d9%85-xgboost-%d8%af%d8%b1-machine-learning/) 

XGBoost. While searching for a groundtaking algorithm specially in competitions like #kaggle everywhere is talking about XGBoost. It is an optimized distributed gradient boosting library for Ensemble learning which combine power of multiple learners.
Bagging and Boosting also was studied today. Planning for getting into kaggle soon.

## [Day 007 - Logistic Regression](https://github.com/soheiltehranipour/100DaysOfMLCode/blob/master/Day%20007-%20Logistic%20Regression.ipynb) 

Using Iris Dataset, I made a project with logistic regression.

## [Day 008 - Feature Scaling](https://github.com/soheiltehranipour/100DaysOfMLCode/blob/master/Day%20007-%20Logistic%20Regression.ipynb) 

Feature scaling is a technique for normalizing the range of independent variables or data components. It is also known as data normalization in data processing and is often conducted during the data preparation stage. Steps for feature scaling : 1.Import packages 2.Import your dataset via pandas 3.check missing values and impute them 4.Encode categorical data 5.train/test split 6.Scale your data in different parts containing train and test
